{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d0642-759f-4859-b1ed-6f06232d68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "import threading\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader, UnstructuredPowerPointLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20c119-dfcc-4b19-8e78-d43bac24e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the LangChain components...\n",
      "Loading documents and setting up vector database...\n",
      "Loaded 5 pages from C:/Users/COCOCE/Desktop/SRH\\1.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\10.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\11.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\12.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\13.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\14.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\15.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\16.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\17.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\18.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\19.pdf\n",
      "Loaded 4 pages from C:/Users/COCOCE/Desktop/SRH\\2.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\21.pdf\n",
      "Loaded 3 pages from C:/Users/COCOCE/Desktop/SRH\\23.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\24.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\25.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\26.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\28.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\3.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\5.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\6.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\7.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\8.pdf\n",
      "Loaded 1 pages from C:/Users/COCOCE/Desktop/SRH\\9.pdf\n",
      "Loaded 64 pages from C:/Users/COCOCE/Desktop/SRH\\Imfashanyigisho k'ubuzima bwimyororokere.pdf\n",
      "Loaded 2 pages from C:/Users/COCOCE/Desktop/SRH\\SRH_QA_Kinyarwanda_Document.pdf\n",
      "\n",
      "Total documents loaded: 107\n",
      "Starting the server...\n",
      "Flask server running in background at http://localhost:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.93.144.101:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://fd94234b1745.ngrok-free.app\n",
      "Twilio Webhook URL: https://fd94234b1745.ngrok-free.app/bot\n",
      "\n",
      "Configure this URL in your Twilio WhatsApp Sandbox.\n",
      "Keep this notebook or script running to maintain the connection.\n",
      "üì© Received message from whatsapp:+250787168817: Muraho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COCOCE\\AppData\\Local\\Temp\\ipykernel_47196\\3700280539.py:95: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
      "127.0.0.1 - - [09/Jul/2025 16:08:51] \"POST /bot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úâÔ∏è Sent response: Muraho neza! Nitwa Umujyanama w‚ÄôUbuzima bw‚ÄôImyororokere. Niteguye kugufasha. Mbwira ikibazo cyangwa ...\n",
      "üì© Received message from whatsapp:+250787168817: Mbwira uko nakwirinda gusama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jul/2025 16:09:32] \"POST /bot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úâÔ∏è Sent response: Umukobwa ashobora kwirinda gusama mu buryo bukurikira: \n",
      "\n",
      "1. Kwifata (kudakora imibonano mpuzabitsina...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize conversation history dictionary to track conversations by user\n",
    "user_memories = {}\n",
    "\n",
    "# Load documents and prepare vector database\n",
    "def setup_vectorstore():\n",
    "    print(\"Loading documents and setting up vector database...\")\n",
    "    # Get all folders inside the main directory\n",
    "    folders = glob.glob('C:/Users/COCOCE/Desktop/SRH')\n",
    "    documents = []\n",
    "    \n",
    "    def get_loader(file_path):\n",
    "        \"\"\"Selects the appropriate loader based on file type.\"\"\"\n",
    "        ext = os.path.splitext(file_path)[-1].lower()\n",
    "        if ext == \".txt\":\n",
    "            return TextLoader(file_path, encoding=\"utf-8\")\n",
    "        elif ext == \".pptx\":\n",
    "            return UnstructuredPowerPointLoader(file_path)\n",
    "        elif ext == \".pdf\":\n",
    "            return PyMuPDFLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "    \n",
    "    # Loop through each folder\n",
    "    for folder in folders:\n",
    "        doc_type = os.path.basename(folder)\n",
    "        \n",
    "        # Get all files inside the folder\n",
    "        files = glob.glob(os.path.join(folder, '**/*'), recursive=True)\n",
    "        \n",
    "        for file_path in files:\n",
    "            if os.path.isfile(file_path):  # Ensure it's a file, not a directory\n",
    "                try:\n",
    "                    loader = get_loader(file_path)\n",
    "                    file_docs = loader.load()\n",
    "                    # Add metadata and store documents\n",
    "                    for doc in file_docs:\n",
    "                        doc.metadata['doc_type'] = doc_type\n",
    "                        documents.append(doc)\n",
    "                    print(f\"Loaded {len(file_docs)} pages from {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Initialize the FAISS vector store\n",
    "    faiss_vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    \n",
    "    # Create the retriever\n",
    "    retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "print(\"Setting up the LangChain components...\")\n",
    "# Create retriever\n",
    "retriever = setup_vectorstore()\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Function to get or create user-specific conversation chain\n",
    "def get_conversation_chain(user_id):\n",
    "    if user_id not in user_memories:\n",
    "        # Set up new conversation memory for this user\n",
    "        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "        user_memories[user_id] = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm, retriever=retriever, memory=memory\n",
    "        )\n",
    "    \n",
    "    return user_memories[user_id]\n",
    "\n",
    "# Chat function that will be called by the Twilio webhook\n",
    "def chat(message, history):\n",
    "    # Extract only the user_id from the history\n",
    "    # This is a simple implementation - you might want to extract the actual user_id from Twilio\n",
    "    user_id = str(id(history))\n",
    "    \n",
    "    # Get the conversation chain for this user\n",
    "    conversation_chain = get_conversation_chain(user_id)\n",
    "    \n",
    "    # Get response from LangChain\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "@app.route(\"/bot\", methods=[\"POST\"])\n",
    "def bot():\n",
    "    # Get the incoming message and sender's phone number\n",
    "    incoming_msg = request.values.get(\"Body\", \"\").strip()\n",
    "    sender = request.values.get(\"From\", \"\")\n",
    "    \n",
    "    print(f\"üì© Received message from {sender}: {incoming_msg}\")\n",
    "    \n",
    "    # Initialize or retrieve conversation history for this user\n",
    "    if sender not in user_memories:\n",
    "        # This initializes the conversation chain for this user\n",
    "        get_conversation_chain(sender)\n",
    "    \n",
    "    try:\n",
    "        # Get the response from your conversation chain\n",
    "        conversation_chain = get_conversation_chain(sender)\n",
    "        result = conversation_chain.invoke({\"question\": incoming_msg})\n",
    "        response = result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing message: {e}\")\n",
    "        response = \"Sorry, I encountered an error processing your message. Please try again.\"\n",
    "    \n",
    "    # Create a Twilio response\n",
    "    twilio_response = MessagingResponse()\n",
    "    twilio_response.message(response)\n",
    "    \n",
    "    print(f\"‚úâÔ∏è Sent response: {response[:100]}...\" if len(response) > 100 else f\"‚úâÔ∏è Sent response: {response}\")\n",
    "    \n",
    "    return str(twilio_response)\n",
    "\n",
    "# Simple homepage route to check if server is live\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"WhatsApp RAG Chatbot is running!\"\n",
    "\n",
    "# Function to run Flask server in the background\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "print(\"Starting the server...\")\n",
    "# Start Flask server in background thread\n",
    "flask_thread = threading.Thread(target=run_flask)\n",
    "flask_thread.daemon = True\n",
    "flask_thread.start()\n",
    "print(\"Flask server running in background at http://localhost:5000\")\n",
    "\n",
    "# Set up ngrok tunnel to expose Flask app for Twilio\n",
    "try:\n",
    "    public_url = ngrok.connect(5000).public_url\n",
    "    print(f\"Public URL: {public_url}\")\n",
    "    print(f\"Twilio Webhook URL: {public_url}/bot\")\n",
    "    print(\"\\nConfigure this URL in your Twilio WhatsApp Sandbox.\")\n",
    "    print(\"Keep this notebook or script running to maintain the connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up ngrok: {e}\")\n",
    "    print(\"You'll need to manually set up a way to expose your local server.\")\n",
    "\n",
    "# Keep the script running to maintain the connection\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)  # Keep the script running\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Server shutting down...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
